@book{10.5555/3312046,
author = {Sutton, Richard S. and Barto, Andrew G.},
title = {Reinforcement Learning: An Introduction},
year = {2018},
isbn = {0262039249},
publisher = {A Bradford Book},
address = {Cambridge, MA, USA},
}

@misc{perringilbert2024afuactorfreecriticupdates,
      title={AFU: Actor-Free critic Updates in off-policy RL for continuous control}, 
      author={Nicolas Perrin-Gilbert},
      year={2024},
      eprint={2404.16159},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2404.16159}, 
}

@misc{fujimoto2018addressingfunctionapproximationerror,
      title={Addressing Function Approximation Error in Actor-Critic Methods}, 
      author={Scott Fujimoto and Herke van Hoof and David Meger},
      year={2018},
      eprint={1802.09477},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1802.09477}, 
}

@misc{kuznetsov2020controllingoverestimationbiastruncated,
      title={Controlling Overestimation Bias with Truncated Mixture of Continuous Distributional Quantile Critics}, 
      author={Arsenii Kuznetsov and Pavel Shvechikov and Alexander Grishin and Dmitry Vetrov},
      year={2020},
      eprint={2005.04269},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2005.04269}, 
}

@misc{dabney2017distributionalreinforcementlearningquantile,
      title={Distributional Reinforcement Learning with Quantile Regression}, 
      author={Will Dabney and Mark Rowland and Marc G. Bellemare and Rémi Munos},
      year={2017},
      eprint={1710.10044},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1710.10044}, 
}

@misc{cetin2023learningpessimismrobustefficient,
      title={Learning Pessimism for Robust and Efficient Off-Policy Reinforcement Learning}, 
      author={Edoardo Cetin and Oya Celiktutan},
      year={2023},
      eprint={2110.03375},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2110.03375}, 
}

@misc{nauman2024overestimationoverfittingplasticityactorcritic,
      title={Overestimation, Overfitting, and Plasticity in Actor-Critic: the Bitter Lesson of Reinforcement Learning}, 
      author={Michal Nauman and Michał Bortkiewicz and Piotr Miłoś and Tomasz Trzciński and Mateusz Ostaszewski and Marek Cygan},
      year={2024},
      eprint={2403.00514},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2403.00514}, 
}

@misc{ji2024seizingserendipityexploitingvalue,
      title={Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy Actor-Critic}, 
      author={Tianying Ji and Yu Luo and Fuchun Sun and Xianyuan Zhan and Jianwei Zhang and Huazhe Xu},
      year={2024},
      eprint={2306.02865},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2306.02865}, 
}

@misc{haarnoja2019softactorcriticalgorithmsapplications,
      title={Soft Actor-Critic Algorithms and Applications}, 
      author={Tuomas Haarnoja and Aurick Zhou and Kristian Hartikainen and George Tucker and Sehoon Ha and Jie Tan and Vikash Kumar and Henry Zhu and Abhishek Gupta and Pieter Abbeel and Sergey Levine},
      year={2019},
      eprint={1812.05905},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1812.05905}, 
}

@misc{moskovitz2022tacticaloptimismpessimismdeep,
      title={Tactical Optimism and Pessimism for Deep Reinforcement Learning}, 
      author={Ted Moskovitz and Jack Parker-Holder and Aldo Pacchiano and Michael Arbel and Michael I. Jordan},
      year={2022},
      eprint={2102.03765},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2102.03765}, 
}

@mastersthesis{pentaliotis2020investigating,
  title={Investigating Overestimation Bias in Reinforcement Learning},
  author={Pentaliotis, Andreas},
  year={2020},
  school={University of Groningen},
  type={Master's Thesis}
}


@article{mnih2015humanlevelcontroldeepreinforcement,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fienberg, Andreas K. and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@article{hasselt2010doubleqlearning,
  title={Double Q-learning},
  author={Van Hasselt, Hado},
  journal={Advances in neural information processing systems},
  volume={23},
  year={2010}
}

@article{lillicrap2015continuouscontroldeepreinforcement,
  title={Continuous control with deep reinforcement learning},
  author={Lillicrap, Timothy P and Hunt, Jonathan J and Pritzel, Alexander and Heess, Nicolas and Erez, Tom and Tassa, Yuval and Silver, David and Wierstra, Daan},
  journal={arXiv preprint arXiv:1509.02971},
  year={2015}
}
  
@article{thrun1993issuesactionselection,
  title={Issues in using function approximation for reinforcement learning},
  author={Thrun, Sebastian and Schwartz, Anton},
  journal={Proceedings of the 1993 connectionist models summer school},
  year={1993}
}
  :
@article{sutton1999policygradientmethodsreinforcement,
  title={Policy gradient methods for reinforcement learning with function approximation},
  author={Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  journal={Advances in neural information processing systems},
  volume={12},
  year={1999}
}
